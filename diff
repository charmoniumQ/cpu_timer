diff --git a/include/cpu_timer.hpp b/include/cpu_timer.hpp
index b08ce03..5aa8c56 100644
--- a/include/cpu_timer.hpp
+++ b/include/cpu_timer.hpp
@@ -50,7 +50,7 @@
 #include "global_state.hpp"
 namespace cpu_timer {
 
-	using Frames = std::deque<detail::Frame>;
+	using Frames = detail::Frames;
 	using Frame = detail::Frame;
 	using CpuNs = detail::CpuTime;
 	using WallNs = detail::WallTime;
@@ -114,3 +114,5 @@ namespace cpu_timer {
 #define CPU_TIMER_TIME_FUNCTION() CPU_TIMER_TIME_FUNCTION_INFO(cpu_timer::type_eraser_default)
 
 #define CPU_TIMER_TIME_EVENT() CPU_TIMER_TIME_EVENT_INFO(false, false, __func__, cpu_timer::type_eraser_default)
+
+#define CPU_TIMER
diff --git a/include/cpu_timer_internal.hpp b/include/cpu_timer_internal.hpp
index 3a9c122..60a6723 100644
--- a/include/cpu_timer_internal.hpp
+++ b/include/cpu_timer_internal.hpp
@@ -182,7 +182,9 @@ namespace detail {
 			;
 	}
 
-	using CallbackType = std::function<void(const Stack&, std::deque<Frame>&&, const std::deque<Frame>&)>;
+	using Frames = std::deque<Frame>;
+
+	using CallbackType = std::function<void(const Stack&, Frames&&, const Frames&)>&;
 
 	class Stack {
 	private:
@@ -194,9 +196,9 @@ namespace detail {
 		const std::thread::id id;
 		const std::thread::native_handle_type native_handle;
 		std::string name;
-		std::deque<Frame> stack;
+		Frames stack;
 		mutable std::mutex finished_mutex;
-		std::deque<Frame> finished; // locked by finished_mutex
+		Frames finished; // locked by finished_mutex
 		size_t index;
 		CpuTime last_log;
 
@@ -285,6 +287,7 @@ namespace detail {
 			, index{0}
 			, last_log{0}
 		{
+			assert(std::this_thread::get_id() == id);
 			enter_stack_frame(nullptr, nullptr, 0, type_eraser_default);
 		}
 
@@ -308,13 +311,16 @@ namespace detail {
 			, finished{std::move(other.finished)}
 			, index{other.index}
 			, last_log{other.last_log}
-		{ }
+		{
+			assert(std::this_thread::get_id() == id);
+		}
 		Stack& operator=(Stack&& other) = delete;
 
 		/**
 		 * @brief Calls callback on a batch containing all completed records, if any.
 		 */
 		void flush() {
+			assert(std::this_thread::get_id() == id);
 			std::lock_guard<std::mutex> finished_lock {finished_mutex};
 			if (!finished.empty()) {
 				// std::lock_guard<std::mutex> config_lock {process.config_mutex};
@@ -340,17 +346,18 @@ namespace detail {
 
 	private:
 		void maybe_flush() {
+			assert(std::this_thread::get_id() == id);
 			std::lock_guard<std::mutex> finished_lock {finished_mutex};
 			// get CPU time is expensive. Instead we look at the last frame
 			if (!finished.empty()) {
 				CpuTime now = finished.back().get_stop_cpu();
 
 				// std::lock_guard<std::mutex> config_lock {process.config_mutex};
-				CpuTime process_log_period = get_log_period();
-				CallbackType process_callback = get_callback();
+				CpuTime callback_period = get_callback_period();
+				CallbackType callback = get_callback();
 
-				if (get_ns(process_log_period) != 0) {
-					if (get_ns(now) == 0 || now > last_log + process_log_period) {
+				if (get_ns(callback_period) != 0) {
+					if (get_ns(now) == 0 || now > last_log + callback_period) {
 						flush_with_locks();
 					}
 				}
@@ -358,7 +365,8 @@ namespace detail {
 		}
 
 		void flush_with_locks() {
-			std::deque<Frame> finished_buffer;
+			assert(std::this_thread::get_id() == id);
+			Frames finished_buffer;
 			finished.swap(finished_buffer);
 			CallbackType callback = get_callback();
 			if (callback) {
@@ -367,25 +375,23 @@ namespace detail {
 		}
 
 		CallbackType get_callback() const;
-		CpuTime get_log_period() const;
+		CpuTime get_callback_period() const;
 		WallTime get_process_start() const;
 	};
 
 	/**
 	 * @brief All stacks in the current process.
 	 *
-	 * This calls callback with one thread's batches of Frames, periodically not sooner than log_period, in the thread whose functions are in the batch.
+	 * This calls callback with one thread's batches of Frames, periodically not sooner than callback_period, in the thread whose functions are in the batch.
 	 */
 	class Process {
 	private:
 		friend class Stack;
 		friend class StackFrameContext;
 
-		// std::atomic<bool> enabled;
-		bool enabled;
-		// std::mutex config_mutex;
+		bool enabled{false};
 		const WallTime start;
-		CpuTime log_period; // locked by config_mutex
+		CpuTime callback_period{0}; // locked by config_mutex
 		CallbackType callback; // locked by config_mutex
 		// Actually, I don't need to lock the config
 		// If two threads race to modify the config, the "winner" is already non-deterministic
@@ -399,9 +405,7 @@ namespace detail {
 	public:
 
 		explicit Process()
-			: enabled{false}
-			, start{wall_now()}
-			, log_period{0}
+			: start{wall_now()}
 		{ }
 
 		WallTime get_start() { return start; }
@@ -425,6 +429,8 @@ namespace detail {
 					std::forward_as_tuple(thread),
 					std::forward_as_tuple(*this, thread, native_handle, std::move(thread_name))
 				);
+				// TODO(grayson5): Consider sending a "courtesy call" that I found a new thread.
+				// thread_to_stack.at(thread).flush();
 			}
 			thread_use_count[thread]++;
 			return thread_to_stack.at(thread);
@@ -456,23 +462,34 @@ namespace detail {
 		}
 
 		/**
-		 * @brief Sets @p log_period for future threads.
-		 *
-		 * All in-progress threads will complete with the prior value.
+		 * @brief Sets @p callback_period for future threads.
 		 */
-		void set_log_period(CpuTime log_period_) {
+		void set_callback_period(CpuTime callback_period_) {
 			// std::lock_guard<std::mutex> config_lock {config_mutex};
-			log_period = log_period_;
+			callback_period = callback_period_;
 		}
 
-		bool is_enabled() {
+		/**
+		 * @brief Calls callback after every frame.
+		 *
+		 * This is usually too inefficient.
+		 */
+		void callback_every_frame() { set_callback_period(CpuTime{1}); }
+
+		/**
+		 * @brief Call callback in destructor.
+		 *
+		 * This is the most efficient, putting the entire lifetime of
+		 * each thread into one batch and calling the callback.
+		 */
+		void callback_once() { set_callback_period(CpuTime{0}); }
+
+		bool is_enabled() const {
 			return enabled;
 		}
 
 		/**
 		 * @brief Sets @p callback for future threads.
-		 *
-		 * All in-progress threads will complete with the prior value.
 		 */
 		void set_callback(CallbackType callback_) {
 			// std::lock_guard<std::mutex> config_lock {config_mutex};
@@ -544,7 +561,7 @@ namespace detail {
 
 	inline CallbackType Stack::get_callback() const { return process.callback; }
 
-	inline CpuTime Stack::get_log_period() const { return process.log_period; }
+	inline CpuTime Stack::get_callback_period() const { return process.callback_period; }
 
 	inline WallTime Stack::get_process_start() const { return process.start; }
 
diff --git a/perf_test/main.cpp b/perf_test/main.cpp
index f2205e7..56f35ac 100644
--- a/perf_test/main.cpp
+++ b/perf_test/main.cpp
@@ -107,7 +107,7 @@ int main() {
 	});
 
 	process.set_enabled(true);
-	process.set_log_period(cpu_timer::CpuNs{0});
+	process.callback_once();
 	process.flush();
 	uint64_t time_logging = exec_in_thread([&] {
 		for (size_t i = 0; i < TRIALS; ++i) {
@@ -119,7 +119,7 @@ int main() {
 	});
 
 	process.set_enabled(true);
-	process.set_log_period(cpu_timer::CpuNs{1});
+	process.callback_every_frame();
 	process.flush();
 	uint64_t time_unbatched = exec_in_thread([&] {
 		for (size_t i = 0; i < TRIALS; ++i) {
@@ -134,7 +134,7 @@ int main() {
 	});
 
 	process.set_enabled(true);
-	process.set_log_period(cpu_timer::CpuNs{0});
+	process.callback_once();
 	process.flush();
 	uint64_t time_thready_logging = exec_in_thread([&] {
 		for (size_t i = 0; i < TRIALS; ++i) {
diff --git a/test/test_cpu_timer.cpp b/test/test_cpu_timer.cpp
index 3123ced..169c24e 100644
--- a/test/test_cpu_timer.cpp
+++ b/test/test_cpu_timer.cpp
@@ -39,7 +39,7 @@ void verify_preorder(const cpu_timer::Frames& trace) {
 		// Check that children are refer to the same parent.
 		if (!frame.is_leaf()) {
 			size_t child_index = frame.get_youngest_callee_index();
-			const cpu_timer::Frame* child;
+			const cpu_timer::Frame* child = nullptr;
 			do {
 				child =  &preorder_trace.at(child_index);
 				EXPECT_EQ(child->get_caller_index(), frame.get_index());
@@ -70,7 +70,7 @@ void verify_preorder(const cpu_timer::Frames& trace) {
 
 			// This asserts that this frame is on the linked-list pointed to by its parent.
 			size_t sibling_index = preorder_trace.at(frame.get_caller_index()).get_youngest_callee_index();
-			const cpu_timer::Frame* sibling;
+			const cpu_timer::Frame* sibling = nullptr;
 			bool found = false;
 			do {
 				sibling = &preorder_trace.at(sibling_index);
@@ -149,7 +149,7 @@ public:
 	Globals() noexcept {
 		auto& proc = cpu_timer::get_process();
 		proc.set_enabled(true);
-		proc.set_log_period(cpu_timer::CpuNs{0});
+		proc.callback_every_frame();
 		proc.set_callback(&err_callback);
 	}
 	~Globals() {
@@ -175,6 +175,7 @@ void verify_any_trace(const cpu_timer::Frames& trace) {
 
 // NOLINTNEXTLINE(hicpp-special-member-functions,cppcoreguidelines-special-member-functions,cppcoreguidelines-owning-memory,cert-err58-cpp,misc-unused-parameters)
 TEST(CpuTimerTest, TraceCorrectBatched) {
+	cpu_timer::get_process().callback_once();
 	cpu_timer::get_process().set_callback([=](const cpu_timer::Stack&, cpu_timer::Frames&& finished, const cpu_timer::Frames& stack) {
 		EXPECT_EQ(0, stack.size());
 		verify_any_trace(finished);
@@ -190,7 +191,7 @@ TEST(CpuTimerTest, TraceCorrectUnbatched) {
 	std::mutex mutex;
 	std::unordered_map<std::thread::id, size_t> count;
 	std::unordered_map<std::thread::id, cpu_timer::Frames> accumulated;
-	cpu_timer::get_process().set_log_period(cpu_timer::CpuNs{1});
+	cpu_timer::get_process().callback_every_frame();
 	cpu_timer::get_process().set_callback([&](const cpu_timer::Stack& stack, cpu_timer::Frames&& finished, const cpu_timer::Frames&) {
 		auto thread = stack.get_id();
 		std::lock_guard<std::mutex> lock{mutex};
